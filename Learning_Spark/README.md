# Learning Spark by Holden Karau etc from O'Reilly.

## [Programming with RDD](https://github.com/mikoSL/Scala_Spark/tree/master/Learning_Spark/Programming_with_RDD)

## Tips:
* quit pyspark: exit()
* quit spark-shell: ctrl + D

* Spark does not require Hadoop, it has support for storage systems implementing Hadoop APIs (including your local filesystem, AWS S3, Cassandra, Hive, HBase, etc).
